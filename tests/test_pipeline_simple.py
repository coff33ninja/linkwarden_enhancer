#!/usr/bin/env python3
"""
Simple test for the Enhancement Pipeline structure

This script tests the pipeline structure without requiring all dependencies.
"""

import asyncio
import sys
from pathlib import Path

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

def test_pipeline_import():
    """Test that the pipeline can be imported"""
    print("ğŸ§ª Testing Pipeline Import")
    print("=" * 30)
    
    try:
        from enhancement.pipeline import (
            EnhancementPipeline, 
            EnhancementConfig, 
            EnhancementOptions, 
            EnhancementMode,
            EnhancementStats,
            EnhancementResult
        )
        
        print("âœ… Successfully imported pipeline classes")
        
        # Test enum values
        print(f"ğŸ“‹ Available modes: {[mode.value for mode in EnhancementMode]}")
        
        # Test configuration creation
        config = EnhancementConfig()
        print(f"âš™ï¸  Default config mode: {config.mode.value}")
        print(f"âš™ï¸  Default batch size: {config.batch_size}")
        
        # Test options creation
        options = EnhancementOptions()
        print(f"ğŸ¯ Default options - enhance titles: {options.enhance_titles}")
        print(f"ğŸ¯ Default options - generate tags: {options.generate_tags}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ Import failed: {e}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return False


def test_pipeline_initialization():
    """Test pipeline initialization"""
    print("\nğŸš€ Testing Pipeline Initialization")
    print("=" * 35)
    
    try:
        from enhancement.pipeline import EnhancementPipeline
        
        # Create minimal config
        config = {
            'enhancement': {'enable_scraping': True},
            'directories': {'data_dir': 'data', 'models_dir': 'models'},
            'progress': {},
            'safety': {}
        }
        
        # Initialize pipeline
        pipeline = EnhancementPipeline(config)
        print("âœ… Pipeline initialized successfully")
        
        # Test stats method
        stats = pipeline.get_pipeline_stats()
        print(f"ğŸ“Š Pipeline stats keys: {list(stats.keys())}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Initialization failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_pipeline_validation():
    """Test bookmark validation"""
    print("\nğŸ” Testing Bookmark Validation")
    print("=" * 32)
    
    try:
        from enhancement.pipeline import EnhancementPipeline
        
        config = {
            'enhancement': {'enable_scraping': True},
            'directories': {'data_dir': 'data', 'models_dir': 'models'},
            'progress': {},
            'safety': {}
        }
        
        pipeline = EnhancementPipeline(config)
        
        # Test valid bookmark
        valid_bookmark = {
            'id': 1,
            'url': 'https://example.com',
            'name': 'Example',
            'tags': []
        }
        
        is_valid = pipeline._is_valid_bookmark(valid_bookmark)
        print(f"âœ… Valid bookmark test: {is_valid}")
        
        # Test invalid bookmark
        invalid_bookmark = {
            'id': 2,
            'url': 'not-a-url',
            'name': 'Invalid'
        }
        
        is_invalid = pipeline._is_valid_bookmark(invalid_bookmark)
        print(f"âŒ Invalid bookmark test: {not is_invalid}")
        
        # Test validation method
        test_bookmarks = [valid_bookmark, invalid_bookmark]
        validated = await pipeline._validate_input_bookmarks(test_bookmarks)
        print(f"ğŸ“‹ Validated {len(validated)}/2 bookmarks")
        
        return True
        
    except Exception as e:
        print(f"âŒ Validation test failed: {e}")
        return False


async def test_quality_assessment():
    """Test quality assessment methods"""
    print("\nğŸ“ Testing Quality Assessment")
    print("=" * 30)
    
    try:
        from enhancement.pipeline import EnhancementPipeline
        
        config = {
            'enhancement': {'enable_scraping': True},
            'directories': {'data_dir': 'data', 'models_dir': 'models'},
            'progress': {},
            'safety': {}
        }
        
        pipeline = EnhancementPipeline(config)
        
        # Test title quality assessment
        good_title = "Complete Guide to Python Programming"
        poor_title = "untitled"
        empty_title = ""
        
        good_score = pipeline._assess_title_quality(good_title)
        poor_score = pipeline._assess_title_quality(poor_title)
        empty_score = pipeline._assess_title_quality(empty_title)
        
        print(f"ğŸ“Š Good title score: {good_score:.2f}")
        print(f"ğŸ“Š Poor title score: {poor_score:.2f}")
        print(f"ğŸ“Š Empty title score: {empty_score:.2f}")
        
        # Test generic title detection
        generic_titles = ["untitled", "page", "home", "index", "document"]
        for title in generic_titles:
            is_generic = pipeline._is_generic_title(title)
            print(f"ğŸ” '{title}' is generic: {is_generic}")
        
        # Test title cleaning
        messy_title = "Great Article - Example.com | News Site"
        cleaned = pipeline._clean_title(messy_title, "https://example.com")
        print(f"ğŸ§¹ Cleaned title: '{messy_title}' â†’ '{cleaned}'")
        
        return True
        
    except Exception as e:
        print(f"âŒ Quality assessment test failed: {e}")
        return False


def test_data_structures():
    """Test data structure creation"""
    print("\nğŸ“¦ Testing Data Structures")
    print("=" * 27)
    
    try:
        from enhancement.pipeline import (
            EnhancementStats, 
            EnhancementResult, 
            EnhancementConfig,
            EnhancementOptions
        )
        
        # Test stats
        stats = EnhancementStats()
        stats.total_bookmarks = 100
        stats.processed_bookmarks = 95
        stats.calculate_success_rate()
        print(f"ğŸ“Š Success rate calculation: {stats.success_rate}%")
        
        # Test result
        result = EnhancementResult(
            success=True,
            enhanced_bookmarks=[],
            stats=stats,
            errors=[],
            warnings=[]
        )
        print(f"âœ… Result created: success={result.success}")
        
        # Test config with custom options
        options = EnhancementOptions(
            enhance_titles=True,
            generate_tags=False,
            max_tags_per_bookmark=5
        )
        
        config = EnhancementConfig(options=options)
        print(f"âš™ï¸  Custom config: enhance_titles={config.options.enhance_titles}")
        print(f"âš™ï¸  Custom config: generate_tags={config.options.generate_tags}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Data structure test failed: {e}")
        return False


async def main():
    """Run all simple tests"""
    print("ğŸ§ª Enhancement Pipeline Simple Test Suite")
    print("=" * 50)
    
    tests = [
        ("Import Test", test_pipeline_import),
        ("Initialization Test", test_pipeline_initialization),
        ("Validation Test", test_pipeline_validation),
        ("Quality Assessment Test", test_quality_assessment),
        ("Data Structures Test", test_data_structures)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ”¬ Running {test_name}")
        try:
            if asyncio.iscoroutinefunction(test_func):
                success = await test_func()
            else:
                success = test_func()
            
            if success:
                passed += 1
                print(f"âœ… {test_name} PASSED")
            else:
                print(f"âŒ {test_name} FAILED")
                
        except Exception as e:
            print(f"âŒ {test_name} ERROR: {e}")
    
    print(f"\nğŸ¯ Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Pipeline structure is working correctly.")
    else:
        print("âš ï¸  Some tests failed. Check the output above for details.")


if __name__ == "__main__":
    asyncio.run(main())